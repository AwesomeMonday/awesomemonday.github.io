I"π7<p>Hello, this is Hao.</p>

<p>As I was looking for more projects to practice, I came across with this virtual experience programe on <a href="https://www.theforage.com/" target="_blank">theforage.com</a>. The project is hosted by Accenture North America, and it‚Äôs called Navigating Numbers (full details can be found <a href="https://www.theforage.com/virtual-internships/prototype/hzmoNKtzvAzXsEqx8/Data-Analytics-Virtual-Experience?ref=9W3J5sbu3KKhpJDGr" target="_blank">here</a>). The goal for this project is to be ‚Äúequipped with data fundamentals and an understanding of what a career in data analytics could look like‚Äù.</p>

<p>The project is devided into 4 modules: project understanding, data cleaning and modeling, data visualization and storytelling, present to the client. Immediately, the flow from understand the business tasks, clean and process the data, to create visualization to gain business insights, all sounded very similar to the Google Data Analytics courses, but with one big different, in order to complete the course and receive the certificate, a video presentation must be recorded and submitted.</p>

<p>Let‚Äôs get started.</p>

<h2 id="company-background">Company Background</h2>

<p>Social Buzz is a social media and content creation company founded in 2010, based in San Francisco. Over the past 5 years, it has reached over 500 million active users monthly. Every day over 100,000 pieces of content, ranging from text, images, videos and GIFs are posted.</p>

<h2 id="business-task">Business Task</h2>

<p>All the data users posted on Social Buzz is highly unstructured and requires extrememly sophisticated and expensive technology to manage and maintain. More than 80% of the employees are technical staff working on maintaining this highly complex technology.
To start our engagement with Social Buzz, we are running a 3-month pilot program to prove to them that we are the best firm to work with to oversee their scaling process effectively.</p>

<h4 id="deliverables">Deliverables</h4>

<ul>
  <li>An audit of their big data practice</li>
  <li>Recommendations for a successful IPO</li>
  <li>An analysis of their content categories that highlights the top 5 categories with largest aggregate popularity</li>
</ul>

<h4 id="my-duty">My Duty</h4>

<p>As the data Analyst, I am primarily responsible for completing the hands-on analysis of data and find the top 5 popular categories of content, then translating the requirements of the project into insights.</p>

<h2 id="prepare-the-data">Prepare the Data</h2>

<p>The team has extracted a set of 7 sample data sets using SQL, my job is to make sense of the data and create my own data set to fulfill the requirements of this task.</p>

<p>I will use RStudio as the tool of choice for this module.</p>

<p>load in the libraries</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">tidyverse</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">Hmisc</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">lubridate</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">janitor</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>load in the 7 data sets provided for this project</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rm</span><span class="p">(</span><span class="n">list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ls</span><span class="p">())</span><span class="w">
</span><span class="n">content</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"https://cdn.theforage.com/vinternships/companyassets/T6kdcdKSTfg2aotxT/Content%20(1).csv"</span><span class="p">)</span><span class="w">

</span><span class="n">location</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"https://cdn.theforage.com/vinternships/companyassets/T6kdcdKSTfg2aotxT/Location%20(1).csv"</span><span class="p">)</span><span class="w">

</span><span class="n">profile</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"https://cdn.theforage.com/vinternships/companyassets/T6kdcdKSTfg2aotxT/Profile%20(1).csv"</span><span class="p">)</span><span class="w">

</span><span class="n">reaction</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"https://cdn.theforage.com/vinternships/companyassets/T6kdcdKSTfg2aotxT/Reactions%20(1).csv"</span><span class="p">)</span><span class="w">

</span><span class="n">reaction_types</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"https://cdn.theforage.com/vinternships/companyassets/T6kdcdKSTfg2aotxT/ReactionTypes%20(1).csv"</span><span class="p">)</span><span class="w">

</span><span class="n">session</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"https://cdn.theforage.com/vinternships/companyassets/T6kdcdKSTfg2aotxT/Session%20(1).csv"</span><span class="p">)</span><span class="w">

</span><span class="n">user</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"https://cdn.theforage.com/vinternships/companyassets/T6kdcdKSTfg2aotxT/User%20(1).csv"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<ol>
  <li>content data set<br />
log: removed unnecessary columns, and clean col names; cleaned category names;
this data set will join with other sets on content_id and user_id</li>
</ol>

<pre><code class="language-{r}"># use describe function to see if there are any missing values
describe(content)

content &lt;- content %&gt;% 
  select(-"URL", -...1) %&gt;% 
  clean_names()

glimpse(content)


# some category has quotes on them and some don't, remove marks and change to lower case to keep consistency

content &lt;- content %&gt;% 
  mutate(category = gsub('[\"]', '', category)) %&gt;% 
  mutate(category = tolower(category)) %&gt;% 
  mutate(category = fct_recode(category, "public_speaking" = "public speaking"),
         category = fct_recode(category, "healthy_eating" = "healthy eating"))

# rename some of the column names to prepare for joining datasets
content &lt;- content %&gt;% 
  rename(content_type = type)

# initial exploration of the 'content' dataset
content %&gt;% 
  count(category, sort = TRUE) %&gt;% 
  View()

# top 5 categories by content category
content %&gt;% 
  mutate(category = fct_lump(category, 5)) %&gt;% 
  group_by(category) %&gt;% 
  summarise(total = n()) %&gt;% 
  arrange(desc(total))

# distribution of categories
content %&gt;% 
  group_by(category) %&gt;% 
  summarise(n = n()) %&gt;% 
  mutate(category = fct_reorder(category, n),) %&gt;% 
  ggplot(aes(category, n, fill = category)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(title = "rank on category by the number of posts", y = 'number of posts', x = '')


# distribution of content_type 
content %&gt;% 
  group_by(content_type) %&gt;% 
  summarise(n = n()) %&gt;% 
  mutate(content_type = fct_reorder(content_type, n)) %&gt;% 
  ggplot(aes(content_type, n, fill = content_type)) +
  geom_col(show.legend = FALSE) +
  coord_flip()+
  labs(title = 'rank on content_type by the number of posts', y = 'number of posts', x = '')
</code></pre>

<p>2.location data set
log: removed unnecessary columns and cleaned col names; extracted zip code and state name from address then dropped address column
this set will join with user data set on user_id to view the state distribution on map</p>

<pre><code class="language-{r}">describe(location)

location &lt;- location %&gt;% 
  select(-...1) %&gt;% 
  clean_names()

#extract state and zip code from address then drop address column
location &lt;- location %&gt;% 
  mutate(zip_code = str_sub(address, -5),
         state = str_sub(address, -8, -7)) %&gt;% 
  select(-address)

# initial exploration of the 2 new columns
location %&gt;% 
  count(state, sort = TRUE) %&gt;% 
  View()
location %&gt;% 
  count(zip_code, sort = TRUE)
glimpse(location)
</code></pre>

<ol>
  <li>profile
log: separated interests into individual rows, removed marks and corrected spelling error, and dropped age column
this set will NOT be used to join with other datasets for the final csv</li>
</ol>

<pre><code class="language-{r}">describe(profile)

profile &lt;- profile %&gt;% 
  select(-'...1', -'Age')

profile &lt;- profile %&gt;% 
  separate_rows(Interests, sep = ',') %&gt;% 
  mutate(Interests = str_replace(Interests, "\\'", "")) %&gt;%   
  mutate(Interests = str_replace(Interests, "\\[", "")) %&gt;%   
  mutate(Interests = str_replace(Interests, "\\]", "")) %&gt;%   
  mutate(Interests = str_replace(Interests, "\\'", "")) %&gt;%   
  mutate(Interests = str_replace(Interests, " ", "")) %&gt;% 
  clean_names() 

# recode some of the misspelled names
profile &lt;- profile %&gt;% 
  mutate(interests = fct_recode(interests, "public_speaking" = "public speaking"),
         interests = fct_recode(interests, "public_speaking" = "publicspeaking"),
         interests = fct_recode(interests, "healthy_eating" = "healthy eating"),
         interests = fct_recode(interests, "healthy_eating" = "healthyeating")) 

profile %&gt;% 
  count(interests, sort = TRUE)
</code></pre>

<ol>
  <li>reaction
log: removed unnecessary columns and cleaned col names; extracted time info for future analysis; 
dropped the user_id column since it refers the user that interacted with the content, not the user who created it; 
this set will join with reaction_types on type to get the scores of each content thru group by</li>
</ol>

<pre><code class="language-{r}">describe(reaction) 

# looking into datetime column, extracting the month, day, weekday and hour info for initial exploration in R
reaction &lt;- reaction %&gt;% 
  mutate(Month = month(Datetime, label = TRUE),
         Day = day(Datetime),
         Weekday = wday(Datetime, label = TRUE),
         Hour = hour(Datetime)) %&gt;% 
  clean_names() %&gt;%
  select(-(x1), -(user_id)) 
  
reaction &lt;- reaction %&gt;% 
  rename(reaction_type = type) 

# reaction type distribution
reaction %&gt;% 
  filter(!is.na(reaction_type)) %&gt;% 
  group_by(reaction_type) %&gt;% 
  summarise(total = n()) %&gt;% 
  mutate(reaction_type = fct_reorder(reaction_type, total)) %&gt;% 
  ggplot(aes(total, reaction_type, fill = reaction_type))+
  geom_col(show.legend = F) +
  labs(title = 'reaction type distribution', x = 'number of reactions', y = '')
 
# initial exploration on the time variables
reaction %&gt;% 
  group_by(month) %&gt;% 
  summarise(total = n()) %&gt;% 
  ggplot(aes(month, total, group = 1))+
  geom_line()+
  expand_limits(y = 0)+
  labs(title = 'reactoin count by month', x = '', y = '')

reaction %&gt;% 
  group_by(weekday) %&gt;% 
  summarise(total = n()) %&gt;% 
  ggplot(aes(weekday, total, group = 1))+
  geom_line()+
  expand_limits(y = 0)+
  labs(title = 'reaction count by weekday', x = '', y = '')

reaction %&gt;% 
  group_by(hour) %&gt;% 
  summarise(total = n()) %&gt;% 
  ggplot(aes(hour, total, group = 1))+
  geom_line()+
  expand_limits(y = 0)+
  labs(title = 'reaction count by hour', x = '', y = '')
</code></pre>

<ol>
  <li>reaction_types
log: removed unnecessary columns and cleaned col names; 
this set will join reaction data set and use the score to rank the top 5 categories that is in the task</li>
</ol>

<pre><code class="language-{r}">reaction_types &lt;- reaction_types %&gt;% 
  select(-"...1")

reaction_types &lt;- reaction_types %&gt;% 
  clean_names() %&gt;%
  rename(reaction_type = type)

# what kind of reaction_types have high scores
reaction_types %&gt;% 
  arrange(desc(score))

# sentiment distribution
reaction_types %&gt;%
  count(sentiment, sort = TRUE)
</code></pre>

<ol>
  <li>session
log: removed unnecessary columns and cleaned names; 
this data set can provide us some interesting insights on device usage, but it‚Äôs not ralevant to the analysis this time</li>
</ol>

<pre><code class="language-{r}">session &lt;- session %&gt;% 
  select(-...1) %&gt;% 
  clean_names()
</code></pre>

<ol>
  <li>user data set
log: removed unnecessary columns and cleaned column names
we don‚Äôt need the user info for our business task</li>
</ol>

<pre><code class="language-{r}">user &lt;- user %&gt;% 
  select(-...1) %&gt;% 
  clean_names()
</code></pre>

<p>since the business task is to find out the top 5 most popular categories, it should be based on the content reactions, but the count of how many posts
of content in each category. I will use the score to calculate the ranks.</p>

<pre><code class="language-{r}"># first let's join the tables

buzz_full &lt;- location %&gt;% 
  right_join(content, by = 'user_id') %&gt;% 
  left_join(reaction, by = 'content_id') %&gt;% 
  left_join(reaction_types, by = 'reaction_type')

# export the dataset for future analysis
# write.csv(buzz_full, "C:\\Users\\haoli\\Desktop\\buzz_full.csv")

# then we can use the aggregated score to rank the categories
buzz_full %&gt;% 
  group_by(category) %&gt;% 
  summarise(total_score = sum(score, na.rm = T)) %&gt;% 
  arrange(desc(total_score))

# the top 5 categories are: animals, science, healthy_eating, technology, food
social_buzz_top5 &lt;- buzz_full %&gt;% 
  filter(category %in% c("animals", "science", "healthy_eating", "technology", "food"))
# write.csv(social_buzz_top5, "C:\\Users\\haoli\\Desktop\\social_buzz_top5.csv")
</code></pre>

:ET